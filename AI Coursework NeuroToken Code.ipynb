{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroToken: Capturing Author Style with LSTM\n",
    "\n",
    "Source code for COM3025 - Vasily Shcherbinin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for Training \n",
    "### Importing all necessary libraries, defining paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import operator\n",
    "import random\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "import pronouncing\n",
    "from pyphonetics import Soundex\n",
    "from transliterate import translit\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "SONNETS = \"./sonnets.txt\"\n",
    "SONNET_CLEAN = \"./sonnets_clean.txt\"\n",
    "TRUMP = \"./speeches.txt\"\n",
    "TRUMP4 = \"./trump4.txt\"\n",
    "\n",
    "WORD_RE = re.compile(r'[a-zA-Z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising Soundex library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundex = Soundex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining helper methods to simplify loading and saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(*args):\n",
    "    for varname in args:\n",
    "        with open(\"{}.txt\".format(varname), \"r\") as f:\n",
    "            yield json.loads(f.read())\n",
    "            \n",
    "def save(**kwargs):\n",
    "    for varname, data in kwargs.items():\n",
    "        with open(\"{}.txt\".format(varname), \"w\") as f:\n",
    "            f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of using the Pronouncing library to tokenise words\n",
    "### Tokenisation by part of speech, location of stressed syllable and Soundex value of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of syllables: 1\n",
      "Stressed syllable:1\n",
      "Relative stress: 1.0\n",
      "WRB2W000\n"
     ]
    }
   ],
   "source": [
    "def partOfSpeech_stress_soundex(word):\n",
    "    if len(word) > 1:\n",
    "        sdx = soundex.phonetics(word)\n",
    "    else:\n",
    "        sdx = word\n",
    "\n",
    "    pronunciation_list = pronouncing.phones_for_word(word)\n",
    "    if (len(pronunciation_list) == 0):\n",
    "        syllable_count = 3\n",
    "        stress = 2\n",
    "    else:\n",
    "        pronouncing.stresses(pronunciation_list[0])\n",
    "\n",
    "        syllable_count = pronouncing.syllable_count(pronunciation_list[0])\n",
    "        print(\"Number of syllables: \" + str(syllable_count))\n",
    "\n",
    "        if (syllable_count == 1):\n",
    "            stress_syllable = 1\n",
    "        else:\n",
    "            stress_syllable = re.search(\"1\", pronouncing.stresses(pronunciation_list[0])).start() + 1\n",
    "        print(\"Stressed syllable:\" + str(stress_syllable))\n",
    "\n",
    "        rel_stress = stress_syllable / syllable_count\n",
    "        print(\"Relative stress: \" + str(rel_stress))\n",
    "\n",
    "        if rel_stress <= 0.5:\n",
    "            stress = 0\n",
    "        elif rel_stress <= 0.8:\n",
    "            stress = 1\n",
    "        else:\n",
    "            stress = 2\n",
    "\n",
    "    text = nltk.word_tokenize(word)\n",
    "\n",
    "    _, partOfSpeech = nltk.pos_tag(text)[0]\n",
    "\n",
    "    return \"{}{}{}\".format(partOfSpeech, stress, sdx)\n",
    "\n",
    "\n",
    "print(partOfSpeech_stress_soundex(\"why\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation by part of speech and Soundex value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBGW635\n"
     ]
    }
   ],
   "source": [
    "def partOfSpeech_soundex(word):\n",
    "    try:\n",
    "        if len(word) > 1:\n",
    "            sdx = soundex.phonetics(word)\n",
    "        else:\n",
    "            sdx = word\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "    text = nltk.word_tokenize(word)\n",
    "\n",
    "    _, partOfSpeech = nltk.pos_tag(text)[0]\n",
    "\n",
    "    return \"{}{}\".format(partOfSpeech, sdx)\n",
    "\n",
    "\n",
    "print(partOfSpeech_soundex(\"Writing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation by syllable count and Soundex value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1T000\n"
     ]
    }
   ],
   "source": [
    "def syllable_soundex(word):\n",
    "    try:\n",
    "        if len(word) > 1:\n",
    "            sdx = soundex.phonetics(word)\n",
    "        else:\n",
    "            sdx = word\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "    syllable_count = 0\n",
    "    pronunciation_list = pronouncing.phones_for_word(word)\n",
    "    if (len(pronunciation_list) == 0):\n",
    "        print(\"No English Word Found\")\n",
    "    else:\n",
    "        syllable_count = pronouncing.syllable_count(pronunciation_list[0])\n",
    "\n",
    "    return \"{}{}\".format(syllable_count, sdx)\n",
    "\n",
    "\n",
    "print(syllable_soundex(\"the\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation by part of speech, syllable count and Soundex value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS1W420\n"
     ]
    }
   ],
   "source": [
    "def partOfSpeech_syllable_soundex(word):\n",
    "    try:\n",
    "        if len(word) > 1:\n",
    "            sdx = soundex.phonetics(word)\n",
    "        else:\n",
    "            sdx = word\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "    syllable_count = 0\n",
    "    pronunciation_list = pronouncing.phones_for_word(word)\n",
    "    if (len(pronunciation_list) == 0):\n",
    "        print(\"No English Word Found\")\n",
    "    else:\n",
    "        syllable_count = pronouncing.syllable_count(pronunciation_list[0])\n",
    "\n",
    "    text = nltk.word_tokenize(word)\n",
    "\n",
    "    _, partOfSpeech = nltk.pos_tag(text)[0]\n",
    "\n",
    "    return \"{}{}{}\".format(partOfSpeech, syllable_count, sdx)\n",
    "\n",
    "print(partOfSpeech_syllable_soundex(\"walls\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation by location of stressed syllable and Soundex value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of syllables: 3\n",
      "Accented syllable: 2\n",
      "Relative accent: 0.6666666666666666\n",
      "1A525\n"
     ]
    }
   ],
   "source": [
    "def stress_soundex(word):\n",
    "    if word == \"/\":\n",
    "        return \"/\"\n",
    "\n",
    "    if len(word) > 1:\n",
    "        sdx = soundex.phonetics(word)\n",
    "    else:\n",
    "        sdx = word\n",
    "\n",
    "    pronunciation_list = pronouncing.phones_for_word(word)\n",
    "    if (len(pronunciation_list) == 0):\n",
    "        print(\"No English Word Found\")\n",
    "    else:\n",
    "        pronouncing.stresses(pronunciation_list[0])\n",
    "\n",
    "        syllable_count = pronouncing.syllable_count(pronunciation_list[0])\n",
    "        print(\"Number of syllables: \" + str(syllable_count))\n",
    "\n",
    "        if (syllable_count == 1):\n",
    "            stress_syllable = 1\n",
    "        else:\n",
    "            stress_syllable = re.search(\"1\", pronouncing.stresses(pronunciation_list[0])).start() + 1\n",
    "            \n",
    "        print(\"Accented syllable: \" + str(stress_syllable))\n",
    "\n",
    "    rel_stress = stress_syllable / syllable_count\n",
    "    print(\"Relative accent: \" + str(rel_stress))\n",
    "\n",
    "    if rel_stress <= 0.5:\n",
    "        stress = 0\n",
    "    elif rel_stress <= 0.8:\n",
    "        stress = 1\n",
    "    else:\n",
    "        stress = 2\n",
    "\n",
    "    return \"{}{}\".format(stress, sdx)\n",
    "\n",
    "\n",
    "print(stress_soundex(\"amazing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pronouncing library to find rhyming words - potentially useful when generating poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_rhyme(word, rhymed_word):\n",
    "    if (rhymed_word in pronouncing.rhymes(word)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(is_rhyme(\"fun\", \"sun\"))\n",
    "print(is_rhyme(\"fall\", \"wall\"))\n",
    "print(is_rhyme(\"billy\", \"silly\"))\n",
    "print(is_rhyme(\"orange\", \"apple\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Levenshtein Distance to find similar words\n",
    "### Levenshtein Distance defines the minimal number of letter substitutions required in order to convert one word/sentence into another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def levenshtein_distance(word1, word2):\n",
    "    if len(word1) < len(word2):\n",
    "        return levenshtein_distance(word2, word1)\n",
    "\n",
    "    if len(word2) == 0:\n",
    "        return len(word1)\n",
    "\n",
    "    previous_row = list(range(len(word2) + 1))\n",
    "\n",
    "    for i, char1 in enumerate(word1):\n",
    "        current_row = [i + 1]\n",
    "\n",
    "        for j, char2 in enumerate(word2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (char1 != char2)\n",
    "\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "print(levenshtein_distance(\"hello\", \"hello\"))\n",
    "print(levenshtein_distance(\"hello\", \"hillo\"))\n",
    "print(levenshtein_distance(\"hello\", \"hubba\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the files to be used - remove whitespace, redundant lines, punctuation and words unrecognised by Pronouncing library (i.e. non-dictionary words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = SONNETS\n",
    "def remove_empty_lines(filename):\n",
    "    \"\"\"Overwrite the file, removing empty lines and lines that contain only whitespace.\"\"\"\n",
    "    with open(filename) as in_file, open(filename, 'r+') as out_file:\n",
    "        out_file.writelines(line for line in in_file if line.strip())\n",
    "        out_file.truncate()\n",
    "        \n",
    "remove_empty_lines(SONNETS)\n",
    "\n",
    "with open(SONNETS, encoding=\"utf8\") as f:\n",
    "    g = open(\"./sonnets_clean.txt\",\"w+\")\n",
    "    for line in f:\n",
    "        cleared_line = \" \".join(WORD_RE.findall(line))\n",
    "        wordList = re.sub(\"[^\\w]\", \" \",  cleared_line).split()\n",
    "        for f in wordList:\n",
    "            pronunciation_list = pronouncing.phones_for_word(f)\n",
    "            if(len(pronunciation_list) != 0):\n",
    "                g.write(f.lower()+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRUMP, encoding=\"utf8\") as f:\n",
    "    g = open(TRUMP4,\"w+\")\n",
    "    for line in f:\n",
    "        cleared_line = \" \".join(WORD_RE.findall(line))\n",
    "        wordList = re.sub(\"[^\\w]\", \" \",  cleared_line).split()\n",
    "        for f in wordList:\n",
    "            pronunciation_list = pronouncing.phones_for_word(f)\n",
    "            if(len(pronunciation_list) != 0):\n",
    "                g.write(f.lower()+\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Corpuses\n",
    "### For the Text whose style we would like to learn, generate several different style corpuses - any of those can then be used for training the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech', 'thank', 'you', 'so', 'much', 'that', 's', 'so', 'nice', 't', 'he', 'a', 'great', 'guy', 'he', 't', 'get', 'a', 'fair', 'press', 'he', 't', 'get', 'it', 'it', 's', 'just', 'not', 'fair', 'and', 'i', 'have', 'to', 'tell', 'you', 'i', 'm', 'here', 'and', 'very', 'strongly', 'here', 'because', 'i', 'have', 'great', 'respect', 'for', 'steve', 'king', 'and', 'have', 'great', 'respect', 'likewise', 'for', 'citizens', 'united', 'david', 'and', 'everybody', 'and', 'tremendous', 'for', 'the', 'tea', 'party', 'also', 'also', 'the', 'people', 'of', 'iowa', 'they', 'have', 'something', 'in', 'common', 'hard', 'working', 'people', 'they', 'want', 'to', 'work', 'they', 'want', 'to', 'make', 'the', 'country', 'great', 'i', 'love', 'the', 'people', 'of', 'iowa', 'so', 'that']\n",
      "['NNS120', 'NNT520', 'PRPY000', 'RBS000', 'JJM200', 'INT300', 'NNs', 'RBS000', 'JJN200', 'NNt', 'PRPH000', 'DTa', 'JJG630', 'NNG000', 'PRPH000', 'NNt', 'VBG300', 'DTa', 'NNF600', 'NNP620', 'PRPH000', 'NNt', 'VBG300', 'PRPI300', 'PRPI300', 'NNs', 'RBJ230', 'RBN300', 'NNF600', 'CCA530', 'NNi', 'VBH100', 'TOT000', 'NNT400', 'PRPY000', 'NNi', 'NNm', 'RBH600', 'CCA530', 'RBV600', 'RBS365', 'RBH600', 'INB220', 'NNi', 'VBH100', 'JJG630', 'NNR212', 'INF600', 'NNS310', 'NNK520', 'CCA530', 'VBH100', 'JJG630', 'NNR212', 'RBL220', 'INF600', 'NNSC325', 'JJU533', 'NND130', 'CCA530', 'NNE161', 'CCA530', 'JJT655', 'INF600', 'DTT000', 'NNT000', 'NNP630', 'RBA420', 'RBA420', 'DTT000', 'NNSP140', 'INO100', 'NNI000', 'PRPT000', 'VBH100', 'NNS535', 'INI500', 'JJC550', 'JJH630', 'VBGW625', 'NNSP140', 'PRPT000', 'NNW530', 'TOT000', 'NNW620', 'PRPT000', 'NNW530', 'TOT000', 'VBM200', 'DTT000', 'NNC536', 'JJG630', 'NNi', 'NNL100', 'DTT000', 'NNSP140', 'INO100', 'NNI000', 'RBS000', 'INT300']\n",
      "['1S120', '1T520', '1Y000', '1S000', '1M200', '1T300', '1s', '1S000', '1N200', '1t', '1H000', '1a', '1G630', '1G000', '1H000', '1t', '1G300', '1a', '1F600', '1P620', '1H000', '1t', '1G300', '1I300', '1I300', '1s', '1J230', '1N300', '1F600', '1A530', '1i', '1H100', '1T000', '1T400', '1Y000', '1i', '1m', '1H600', '1A530', '2V600', '2S365', '1H600', '2B220', '1i', '1H100', '1G630', '2R212', '1F600', '1S310', '1K520', '1A530', '1H100', '1G630', '2R212', '2L220', '1F600', '3C325', '3U533', '2D130', '1A530', '4E161', '1A530', '3T655', '1F600', '1T000', '1T000', '2P630', '2A420', '2A420', '1T000', '2P140', '1O100', '3I000', '1T000', '1H100', '2S535', '1I500', '2C550', '1H630', '2W625', '2P140', '1T000', '1W530', '1T000', '1W620', '1T000', '1W530', '1T000', '1M200', '1T000', '2C536', '1G630', '1i', '1L100', '1T000', '2P140', '1O100', '3I000', '1S000', '1T300']\n",
      "['NN1S120', 'NN1T520', 'PRP1Y000', 'RB1S000', 'JJ1M200', 'IN1T300', 'NN1s', 'RB1S000', 'JJ1N200', 'NN1t', 'PRP1H000', 'DT1a', 'JJ1G630', 'NN1G000', 'PRP1H000', 'NN1t', 'VB1G300', 'DT1a', 'NN1F600', 'NN1P620', 'PRP1H000', 'NN1t', 'VB1G300', 'PRP1I300', 'PRP1I300', 'NN1s', 'RB1J230', 'RB1N300', 'NN1F600', 'CC1A530', 'NN1i', 'VB1H100', 'TO1T000', 'NN1T400', 'PRP1Y000', 'NN1i', 'NN1m', 'RB1H600', 'CC1A530', 'RB2V600', 'RB2S365', 'RB1H600', 'IN2B220', 'NN1i', 'VB1H100', 'JJ1G630', 'NN2R212', 'IN1F600', 'NN1S310', 'NN1K520', 'CC1A530', 'VB1H100', 'JJ1G630', 'NN2R212', 'RB2L220', 'IN1F600', 'NNS3C325', 'JJ3U533', 'NN2D130', 'CC1A530', 'NN4E161', 'CC1A530', 'JJ3T655', 'IN1F600', 'DT1T000', 'NN1T000', 'NN2P630', 'RB2A420', 'RB2A420', 'DT1T000', 'NNS2P140', 'IN1O100', 'NN3I000', 'PRP1T000', 'VB1H100', 'NN2S535', 'IN1I500', 'JJ2C550', 'JJ1H630', 'VBG2W625', 'NNS2P140', 'PRP1T000', 'NN1W530', 'TO1T000', 'NN1W620', 'PRP1T000', 'NN1W530', 'TO1T000', 'VB1M200', 'DT1T000', 'NN2C536', 'JJ1G630', 'NN1i', 'NN1L100', 'DT1T000', 'NNS2P140', 'IN1O100', 'NN3I000', 'RB1S000', 'IN1T300']\n"
     ]
    }
   ],
   "source": [
    "style_corpus = []\n",
    "with open(TRUMP4, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        cleaned_line = \" \".join(WORD_RE.findall(line))\n",
    "        if cleaned_line:\n",
    "            style_corpus += [w for w in cleaned_line.lower().split(\" \") if w and not w.isnumeric()]\n",
    "#             style_corpus.append(\"/\")\n",
    "\n",
    "print(style_corpus[:100])\n",
    "\n",
    "style_corpus_soundex = {}\n",
    "style_corpus_soundex_list = []\n",
    "for w in style_corpus:\n",
    "    pronunciation_list = pronouncing.phones_for_word(w)\n",
    "    if (len(pronunciation_list) == 0):\n",
    "        w = \"\"\n",
    "    else:\n",
    "        sdx = partOfSpeech_soundex(w)\n",
    "        style_corpus_soundex_list.append(sdx)\n",
    "        style_corpus_soundex[sdx] = w\n",
    "\n",
    "print(style_corpus_soundex_list[:100])\n",
    "\n",
    "style_corpus_syllables = {}\n",
    "style_corpus_syllables_list = []\n",
    "for w in style_corpus:\n",
    "    sdx = syllable_soundex(w)\n",
    "    style_corpus_syllables_list.append(sdx)\n",
    "    style_corpus_syllables[sdx] = w\n",
    "\n",
    "print(style_corpus_syllables_list[:100])\n",
    "\n",
    "style_corpus_partOfSpeech_syllables = {}\n",
    "style_corpus_partOfSpeech_syllables_list = []\n",
    "for w in style_corpus:\n",
    "    sdx = partOfSpeech_syllable_soundex(w)\n",
    "    style_corpus_partOfSpeech_syllables_list.append(sdx)\n",
    "    style_corpus_partOfSpeech_syllables[sdx] = w\n",
    "\n",
    "print(style_corpus_partOfSpeech_syllables_list[:100])\n",
    "\n",
    "save(\n",
    "    style_corpus=style_corpus,\n",
    "    style_corpus_soundex=style_corpus_soundex,\n",
    "    style_corpus_soundex_list=style_corpus_soundex_list,\n",
    "    style_corpus_syllables=style_corpus_syllables,\n",
    "    style_corpus_syllables_list=style_corpus_syllables_list,\n",
    "    style_corpus_partOfSpeech_syllables=style_corpus_partOfSpeech_syllables,\n",
    "    style_corpus_partOfSpeech_syllables_list=style_corpus_partOfSpeech_syllables_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the other text, generate similar corpuses as above to be used as a dictionary when converting LSTM-generated tokens (result) into plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confused', 'quickly', 'boats', 'throws', 'reference', 'stephanopoulos', 'fundamental', 'aggression', 'dropped', 'newspaper', 'manager', 'unacceptable', 'collar', 'promise', 'brad', 'breakfast', 'restrict', 'christmas', 'lieutenant', 'whose', 'jokingly', 'saving', 'face', 'against', 'bridges', 'blowup', 'disgusting', 'make', 'calling', 'airlines', 'everything', 'theme', 'facilities', 'contribute', 'prayer', 'noble', 'burning', 'tubes', 'join', 'discussing', 'pander', 'obama', 'devaluations', 'congratulations', 'dwight', 'general', 'pipe', 'situation', 'states', 'negotiating', 'sees', 'mile', 'funded', 'fly', 'longer', 'preparation', 'help', 'fields', 'lens', 'speaker', 'bunch', 'nutshell', 'amendment', 'enrichment', 'shipped', 'silicon', 'refer', 'exorbitant', 'plateau', 'mexican', 'producer', 'afterwards', 'authorizes', 'formed', 'ambulances', 'doctors', 'women', 'stanford', 'blah', 'deadly', 'building', 'loaded', 'dismantle', 'evening', 'keeps', 'publishers', 'towns', 'controlled', 'affiliated', 'insider', 'brian', 'pageant', 'matter', 'carter', 'expectation', 'eradicated', 'labor', 'ridiculous', 'compared', 'tournament']\n",
      "['VBNC512', 'RBQ240', 'NNSB320', 'NNST620', 'NNR165', 'NNS315', 'JJF535', 'NNA262', 'VBDD613', 'NNN211', 'NNM526', 'JJU521', 'NNC460', 'NNP652', 'NNB630', 'NNB621', 'NNR236', 'NNC623', 'NNL355', 'WP$W200', 'RBJ252', 'VBGS152', 'NNF200', 'INA252', 'NNSB632', 'NNB410', 'VBGD223', 'VBM200', 'VBGC452', 'NNSA645', 'NNE163', 'NNT500', 'NNSF243', 'NNC536', 'NNP660', 'JJN140', 'NNB655', 'NNST120', 'NNJ500', 'VBGD225', 'NNP536', 'NNO150', 'NNSD143', 'NNSC526', 'NND230', 'JJG564', 'NNP100', 'NNS335', 'NNSS332', 'VBGN233', 'NNSS200', 'NNM400', 'VBNF533', 'NNF400', 'NNL526', 'NNP616', 'NNH410', 'NNSF432', 'NNSL520', 'NNS126', 'NNB520', 'NNN324', 'NNA553', 'NNE562', 'VBNS130', 'NNS425', 'NNR160', 'NNE261', 'NNP430', 'JJM225', 'NNP632', 'NNSA136', 'NNSA362', 'VBNF653', 'NNSA514', 'NNSD236', 'NNSW550', 'NNS351', 'NNB400', 'RBD340', 'NNB435', 'VBNL330', 'NND255', 'VBGE155', 'NNSK120', 'NNSP142', 'NNST520', 'VBNC536', 'VBNA143', 'NNI523', 'NNB650', 'NNP253', 'NNM360', 'NNC636', 'NNE212', 'VBNE632', 'NNL160', 'JJR324', 'VBNC516', 'NNT655']\n",
      "['2C512', '2Q240', '1B320', '1T620', '3R165', '5S315', '4F535', '3A262', '1D613', '3N211', '3M526', '5U521', '2C460', '2P652', '1B630', '2B621', '2R236', '2C623', '3L355', '1W200', '3J252', '2S152', '1F200', '2A252', '2B632', '2B410', '3D223', '1M200', '2C452', '2A645', '3E163', '1T500', '4F243', '3C536', '1P660', '2N140', '2B655', '1T120', '1J500', '3D225', '2P536', '3O150', '5D143', '5C526', '1D230', '3G564', '1P100', '4S335', '1S332', '5N233', '1S200', '1M400', '2F533', '1F400', '2L526', '4P616', '1H410', '1F432', '1L520', '2S126', '1B520', '2N324', '3A553', '3E562', '1S130', '3S425', '2R160', '4E261', '2P430', '3M225', '3P632', '3A136', '4A362', '1F653', '4A514', '2D236', '2W550', '2S351', '1B400', '2D340', '2B435', '2L330', '3D255', '2E155', '1K120', '3P142', '1T520', '2C536', '5A143', '3I523', '2B650', '2P253', '2M360', '2C636', '4E212', '5E632', '2L160', '4R324', '2C516', '3T655']\n",
      "['VBN2C512', 'RB2Q240', 'NNS1B320', 'NNS1T620', 'NN3R165', 'NN5S315', 'JJ4F535', 'NN3A262', 'VBD1D613', 'NN3N211', 'NN3M526', 'JJ5U521', 'NN2C460', 'NN2P652', 'NN1B630', 'NN2B621', 'NN2R236', 'NN2C623', 'NN3L355', 'WP$1W200', 'RB3J252', 'VBG2S152', 'NN1F200', 'IN2A252', 'NNS2B632', 'NN2B410', 'VBG3D223', 'VB1M200', 'VBG2C452', 'NNS2A645', 'NN3E163', 'NN1T500', 'NNS4F243', 'NN3C536', 'NN1P660', 'JJ2N140', 'NN2B655', 'NNS1T120', 'NN1J500', 'VBG3D225', 'NN2P536', 'NN3O150', 'NNS5D143', 'NNS5C526', 'NN1D230', 'JJ3G564', 'NN1P100', 'NN4S335', 'NNS1S332', 'VBG5N233', 'NNS1S200', 'NN1M400', 'VBN2F533', 'NN1F400', 'NN2L526', 'NN4P616', 'NN1H410', 'NNS1F432', 'NNS1L520', 'NN2S126', 'NN1B520', 'NN2N324', 'NN3A553', 'NN3E562', 'VBN1S130', 'NN3S425', 'NN2R160', 'NN4E261', 'NN2P430', 'JJ3M225', 'NN3P632', 'NNS3A136', 'NNS4A362', 'VBN1F653', 'NNS4A514', 'NNS2D236', 'NNS2W550', 'NN2S351', 'NN1B400', 'RB2D340', 'NN2B435', 'VBN2L330', 'NN3D255', 'VBG2E155', 'NNS1K120', 'NNS3P142', 'NNS1T520', 'VBN2C536', 'VBN5A143', 'NN3I523', 'NN2B650', 'NN2P253', 'NN2M360', 'NN2C636', 'NN4E212', 'VBN5E632', 'NN2L160', 'JJ4R324', 'VBN2C516', 'NN3T655']\n"
     ]
    }
   ],
   "source": [
    "dictionary_content_corpus = []\n",
    "with open(TRUMP4, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        cleared_line = \" \".join(WORD_RE.findall(line))\n",
    "        if cleared_line:\n",
    "            dictionary_content_corpus += [w for w in cleared_line.lower().split(\" \") if w and len(w) > 1 and not w.isnumeric()]\n",
    "\n",
    "dictionary_content_corpus = list(set(dictionary_content_corpus))\n",
    "print(dictionary_content_corpus[:100])\n",
    "dictionary_content_corpus_soundex = {partOfSpeech_soundex(w): w for w in dictionary_content_corpus if w}\n",
    "print(list(dictionary_content_corpus_soundex.keys())[:100])\n",
    "dictionary_content_corpus_syllables = {syllable_soundex(w): w for w in dictionary_content_corpus if w}\n",
    "print(list(dictionary_content_corpus_syllables.keys())[:100])\n",
    "dictionary_content_corpus_partOfSpeech_syllables = {partOfSpeech_syllable_soundex(w): w for w in dictionary_content_corpus if w}\n",
    "print(list(dictionary_content_corpus_partOfSpeech_syllables.keys())[:100])\n",
    "\n",
    "save(\n",
    "    dictionary_content_corpus=dictionary_content_corpus, \n",
    "    dictionary_content_corpus_soundex=dictionary_content_corpus_soundex,\n",
    "    dictionary_content_corpus_syllables=dictionary_content_corpus_syllables,\n",
    "    dictionary_content_corpus_partOfSpeech_syllables=dictionary_content_corpus_partOfSpeech_syllables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using LSTM with style corpus as input\n",
    "### Choose between LSTM and Bidirectional LSTM - simply comment out as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 57\n",
      "nb sequences: 460369\n",
      "Vectorization...\n",
      "Building model...\n",
      "WARNING:tensorflow:From C:\\Users\\Vas-DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "WARNING:tensorflow:From C:\\Users\\Vas-DELL\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      " 23424/460369 [>.............................] - ETA: 2:19 - loss: 2.5332"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-8efb5edfbbd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     model.fit(X, y,\n\u001b[0;32m     57\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m               epochs=150)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \" \".join(style_corpus_partOfSpeech_syllables_list)\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('Building model...')\n",
    "# Initialize Sequential Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "# Add the output layer that is a softmax of the number of characters\n",
    "model.add(Dense(len(chars), activation='softmax')) \n",
    "# Optimization through RMSprop\n",
    "optimizer_new = RMSprop() \n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer_new) \n",
    "    \n",
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "# model.add(Bidirectional(LSTM(128, input_shape=(maxlen, len(chars)))))\n",
    "# model.add(Dense(len(chars)))\n",
    "# model.add(Activation('softmax'))\n",
    "# optimizer_new = RMSprop() \n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer_new)\n",
    "\n",
    "def sample(predictions, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 2):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=150)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = '' \n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(300):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            predictions = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(predictions, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('TrumpOnlyBiDir-128.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and see model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 128)               95232     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 57)                7353      \n",
      "=================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"models\\\\TrumpOnly-128.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper method to randomly generate tokens from model -> allows to enter seed if such is known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vas-DELL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N1S353', 'RB1T600', 'IN1W300', 'NN3B314', 'NN3E215', 'JJ1L230', 'NN1M530', 'IN1T300', 'NN1s', 'RB1S500', 'NN1i', 'MD1W400', 'RB2E160', 'NN1T400', 'PRP1Y000', 'RB1S000', 'NN1L200', 'NN3F463', 'RB1S000', 'NN1i', 'VBN1C130', '2VSaB6M25', 'NN1C520', 'CC1A530', 'VB2R400', 'NN1T651', 'IN1I100', 'NN1i', 'NN1D500', 'NN1t', 'VB1D000', 'DT1a', 'NN1C630', 'IN1O100', 'PRP1I300', 'NN1i', 'VBD1W200', 'IN1L200', 'DT1T000', 'JJ1F600', 'NN1H300', 'DT1T000', 'NN3E423', 'IN1T300', 'NN1i', 'VB1L300', 'NN1i', 'VBD1H300', 'JJ1G630', 'JJ3N264', 'NN4D526', 'CC1A530', 'NN1i', 'NN1D500', 'NN1t', 'NN1W530', 'TO1T000', 'VB1M200', 'CD2T645', 'PRP1W000', 'VB1H100', 'TO1T000', 'NN1P300', 'NN5O125', 'PRP1W000', 'NN1V000', 'NNS3P535', 'PRP$2O600', 'NNS2P420', 'PRP1T000', 'VB1H100', 'IN1T300', 'NN3C535', 'TO1T000', 'VB1K100', 'PRP1I300', 'IN2B220', 'IN1I100', 'RB2V600', 'RB1S000', 'RB1S000', 'NN1i', 'NN1R200', 'PRP1I300', 'RB1U100', 'PRP1H000', 'NN1s', 'VBN2S123', 'TO1T000', 'VB1B000', 'RB1S000', 'RB3C630', 'VBG2D420', 'IN1T300', 'RB1S000', 'CC1A530', 'RB1S000', 'DT1T200', 'NN1i', 'NN1D500', 'NN1t', 'NN1T520', 'NN3A535', 'IN1L200', 'PRP1I300', 'IN2B220', 'PRP1W000', 'VB1B000', 'NN4N233', 'IN1F600', 'DT1T000', 'NN2P435', 'NN1i', 'VBD1B230', 'JJ2F625', 'NNS2P140', 'WRB1W000', 'NN1t', 'PRP1W000', 'NN1P300', 'RB1U100', 'PRP1I300', 'VBD1H300', 'NNS3C625', 'CC1B300', 'NN2W550', 'RB1N300', 'PRP1I300', 'VBZ1H200', 'VBN1D500', 'RB3U516', 'NN3W536', 'NN4I516', 'VBZ1I200', 'RB1S000', 'JJ2N230', 'PRP1T000', 'NN1R000', 'VBG2G520', 'TO1T000', 'VB1M200', 'PRP$2O600', 'NN2C536', 'JJ1G630', 'RB2A250', 'NN1i', 'NN1m', 'DT1a', 'NN4P524', 'DT1T200', 'NNS2P140', 'CC1A530', 'PRP1T000', 'NN4R241', 'DT1a', 'NN2T364', 'CC1B300', 'RB1N200', 'NN2S515', 'NN1S120', 'NN2W436', 'VBD1F430', 'PRP1I300', 'VB1G000', 'TO1T000', 'JJ2O360', 'NNS2P422', 'IN1T300', 'PRP1I300', 'NN1s', 'VBG2G520', 'TO1T000', 'VB1M200', 'PRP$2O600', 'NN2C536', 'RB1S000', 'NN1i', 'NN1M500', 'PRP1I300', 'NN1s', 'VBG2G520', 'TO1T000', 'NN1W500', 'PRP1T000', 'NN1D500', 'NN1t', 'NN1B300', 'PRP1H500', 'NN1W000', 'NN1TS52', 'PRP1I300', 'IN2B220', 'PRP1W000', 'VB1H100', 'DT1a', 'JJ1N200', 'NN3A235', 'NN1W000', 'PRP$2O600', 'NNS2A420', 'DT1T000', 'CD1O500', 'NN1O400', 'PRP$1M000', 'NNS1V320', 'PRP1Y000', 'NN1R000', 'RB1N300', 'VBG2W325', 'PRP1I300', 'CC1A530', 'PRP1T000', 'NN1W530', 'PRP1I300', 'NN1i', 'VBN3R123', 'PRP1I300', 'NN1s', 'VBG2G520', 'TO1T000', 'VB1S100', 'PRP1T000', 'NN1W500', 'NN1t', 'VB1D000', 'CC1B300', 'NN1i', 'NN1T520', 'DT1a', 'JJ1L230', 'NN3I000', 'NN1i', 'VBD1W200', 'IN1F600', 'NN2I220', 'CC1A530', 'NN1i', 'NN11230', 'VB1100', 'RB2F636', 'NN2S120', 'WRB1W500', 'DT1T000', 'JJ1L230', 'NN1A230', 'TO1T000', 'RB2E150', 'NN1t', 'sRtR1240', 'NN1i', 'RB1J230', 'VBD1F430', 'NN2V235', 'PRP1I300', 'PRP1I300', 'NN1s', 'VBN2U223', 'IN1F600', 'NN2B100', 'NNS2P']\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"TrumpOnly-128.h5\")\n",
    "def generate_rnn(count):\n",
    "    generated = \"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "\n",
    "    for i in range(count):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "#         next_index = sample(predictions, diversity)\n",
    "        next_index = sample(predictions, 1) #-> comment above, uncomment this if loading existing model\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "content = style_corpus_partOfSpeech_syllables\n",
    "generated = generate_rnn(2000).split(\" \")\n",
    "\n",
    "# seed = \"RB2A450 WDT1W200 CD1T600 NN1T430 RB1N000 RB2A400 VBP1A600 VBN1V500 NNS1T000 NN15000 MD1S400 NN1S500 PRP1I300 RB1S000 PRP1S000 NN2F620 CC1A530 NN1M500 PRP$1Y600 NN1F653 NN1S300 NN2D263 IN2W350 PRP$1H200 NNS1G620 IN1W300 NNS1W632 NNS1M500 IN1W300 NNS2F462 VBN1M500 VBN1S500 MD1S400 NN2F224 NN1S360 NN1S450 NN1C400 NN1B630 IN1I500\"\n",
    "# generated = seed.split(\" \")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From dictionary, using Levenshtein Distance find closest token to the token generated by model and convert to text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stand there with beautiful equipment last month that s soon i will ever tell you so look florida so i caused  change and really trump if i don t do a crowd of it i was like the free hate the election that i let i had great natural democracy and i don t want to make trillion we have to put organization we ve continents our police they have that continue to keep it because if very so so i rush it up he s supposed to be so certainly doing that so and so these i don t thank anything like it because we be negotiate for the building i backed foreign people why t we put up it had citizens but woman not it has done unfairly wonderful infrastructure is so nasty they re going to make our country great again i m a pennsylvania these people and they regulation a twitter but not something spoke waldorf failed it go to other places that it s going to make our country so i mean it s going to win they don t bit him way  it because we have a nice accident way our allies the one oil my vets you re not watching it and they want it i registered it s going to save they win t do but i thank a last iowa i was for isis and i fight have further suppose when the last act to even t  i just failed victim it it s accused for bobby  "
     ]
    }
   ],
   "source": [
    "for term in generated:\n",
    "    guessed_words = {}\n",
    "    for idx, word in content.items():\n",
    "        lev_dist = levenshtein_distance(idx, term)\n",
    "            \n",
    "        if len(word) > 1 and lev_dist <= 1:\n",
    "            guessed_words[word] = lev_dist\n",
    "\n",
    "    if not guessed_words:\n",
    "        for idx, word in style_corpus_soundex.items():\n",
    "            lev_dist = levenshtein_distance(idx, term)\n",
    "            if lev_dist <= 1:\n",
    "                guessed_words[word] = lev_dist\n",
    "    \n",
    "    print(sorted(guessed_words, key=guessed_words.get)[0] if guessed_words else \"\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate results using Multinomial Naive Bayes\n",
    "### Import training and testing sets, create a word count using CountVectorizer and Term Frequency Inverse Document Frequency, then train the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skd\n",
    "categories = ['trump', 'shakespeare']\n",
    "data_train = skd.load_files('C:\\\\Users\\\\Vas-DELL\\\\Documents\\\\NeuroToken\\\\data\\\\train',categories=categories,encoding='ISO-8859-1')\n",
    "data_test = skd.load_files('C:\\\\Users\\\\Vas-DELL\\\\Documents\\\\NeuroToken\\\\data\\\\test',categories=categories,encoding='ISO-8859-1')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "x_train_tf = count_vect.fit_transform(data_train.data)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_tf)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(x_train_tfidf,data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 7975)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tf = count_vect.transform(data_test.data)\n",
    "x_test_tf.shape\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_tf)\n",
    "\n",
    "predicted = clf.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confusion Matrix to demonstrate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " shakespeare       1.00      1.00      1.00        21\n",
      "       trump       1.00      1.00      1.00        21\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[21,  0],\n",
       "       [ 0, 21]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy : \", accuracy_score(data_test.target,predicted))\n",
    "print(metrics.classification_report(data_test.target,predicted,target_names=data_test.target_names))\n",
    "\n",
    "metrics.confusion_matrix(data_test.target,predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
